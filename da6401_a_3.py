# -*- coding: utf-8 -*-
"""DA6401_A-3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/msajid013/da6401-a-3.3e027850-8818-41dc-8248-f55fab02277c.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250520/auto/storage/goog4_request%26X-Goog-Date%3D20250520T173746Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2b2bd80de4aa0acab8e76fe8d5294f029758eb21fae5c482d7ee3e542316c579f69077a40217e3a77289be785474a796c9d3299fb99b7514f068835d369e3090fc6e37113dda7a202731ed0dcd8ce92a2c386d7ea1ffa3abc78137b604d0de1ba37c5a65c044064ed7a51446bd829dc793ff24affbd6415c7f346b10ca6c511e0c1135a147f8bbea2f4ba17099e41563add8a8d1e2c3a8bec1d5062fc3203728d62aac472aa2152184103ca6a568453edbdf533660919fc87949ca0dbe9107ac1fd78666ada35293c2df44d5ed61e8bfbf7680b0b2fa0327ae546f5647e466e65d4e68e1113dfcdb8d99b225b0d78c612c67efe73d75675919e9d834110cae3b
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

msajid013_dakshina_dataset_path = kagglehub.dataset_download('msajid013/dakshina-dataset')

print('Data source import complete.')

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from tqdm import tqdm
import heapq
import csv

import numpy as np
import random
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
import pandas as pd
import wandb

def data_load(path):
    # input - English
    # output - Urdu
    df = pd.read_csv(path,sep='\t', header=None)
    input_data = df[1].tolist()
    output_data = df[0].tolist()
    return input_data, output_data

def create_char_set(train, val):
    char_set = set()
    for word in train:
        for char in str(word):
            char_set.add(char)
    for word in val:
        for char in str(word):
            char_set.add(char)
    return char_set

def check_for_floats(data_list):
    float_values = []
    for item in data_list:
        if isinstance(item, float):
            float_values.append(item)
    return float_values

train_input1, train_output = data_load("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv")
val_input, val_output = data_load("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv")
test_input, test_output = data_load("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv")

print("Number of training samples: ", len(train_input1))
print("Number of validation samples: ", len(val_input))
print("Number of test samples: ", len(test_input))

eng_chars = create_char_set(train_input1, val_input)
print("Total English characters: ",len(eng_chars))
print(sorted(eng_chars))

ur_chars = create_char_set(train_output, val_output)
print("Total Urdu characters: ",len(ur_chars))
print(sorted(ur_chars))

float_values=check_for_floats(train_input1)
print(float_values)

train_input = [x for x in train_input1 if not isinstance(x,float)]
print("Number of training samples: ", len(train_input))

max_seq_eng = len(max(train_input+val_input+test_input, key=len))
max_seq_ur = len(max(train_output+val_output+test_output, key=len))
print("Length of the longest English word in corpus:",max_seq_eng)
print("Length of the longest Urdu word in corpus::",max_seq_ur)

"""eng_chars_idx = {char: idx + 3 for idx, char in enumerate(sorted(eng_chars))}
eng_chars_idx['0'] = 0 # padding
eng_chars_idx['\t'] = 1 # <SOW>
eng_chars_idx['\n'] = 2 # <EOW>
print(eng_chars_idx)
ur_chars_idx = {char: idx+3 for idx, char in enumerate(sorted(ur_chars))}
ur_chars_idx['0'] = 0 # padding
ur_chars_idx['\t'] = 1 # <SOW>
ur_chars_idx['\n'] = 2 # <EOW>
print(ur_chars_idx)
"""

import torch
from torch.utils.data import DataLoader, Dataset
import pandas as pd

def load_data(input_data, output_data, batch_size=32):
    """
    Prepares character-level transliteration data for sequence-to-sequence modeling.

    Args:
        input_data (list or Series): Romanized source words (e.g., Urdu in Latin script).
        output_data (list or Series): Target words in Devanagari/Urdu script.
        batch_size (int): Batch size for the DataLoader.

    Returns:
        dataset (Dataset): Custom PyTorch dataset for transliteration.
        dataloader (DataLoader): DataLoader for iterating over the dataset.
        input_vocab (dict): Character-to-index mapping for input characters.
        target_vocab (dict): Character-to-index mapping for target characters.
        max_input_len (int): Maximum input sequence length.
        max_target_len (int): Maximum target sequence length.
    """

    # Determine the maximum lengths for padding
    max_input_len = len(max(input_data, key=len))
    max_target_len = len(max(output_data, key=len))

    # Initialize character vocabularies with special tokens
    input_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}
    target_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}
    next_index = 3

    # Extract all unique characters from the dataset
    all_input_chars = ''.join(input_data)
    all_target_chars = ''.join(output_data)

    # Create input vocabulary
    for char in sorted(set(all_input_chars)):
        input_vocab[char] = next_index
        next_index += 1

    # Reset index for building target vocabulary
    next_index = 3
    for char in sorted(set(all_target_chars)):
        if char not in target_vocab:
            target_vocab[char] = next_index
            next_index += 1

    # Tokenize input characters and pad to max_input_len
    def tokenize_input(word, vocab, max_len):
        token_ids = [vocab[char] for char in word if char in vocab]
        padded = token_ids[:max_len] + [vocab['<pad>']] * (max_len - len(token_ids))
        return torch.tensor(padded)

    # Tokenize target characters with <sos> and <eos>, then pad
    def tokenize_target(word, vocab, max_len):
        token_ids = [vocab[char] for char in word if char in vocab]
        padded = [vocab['<sos>']] + token_ids[:max_len] + [vocab['<eos>']]
        padded += [vocab['<pad>']] * (max_len + 2 - len(padded))  # +2 for <sos> and <eos>
        return torch.tensor(padded)

    # Define a custom dataset for transliteration pairs
    class TransliterationDataset(Dataset):
        def __init__(self, input_words, target_words, input_vocab, target_vocab, max_input_len, max_target_len):
            self.input_words = input_words
            self.target_words = target_words
            self.input_vocab = input_vocab
            self.target_vocab = target_vocab
            self.max_input_len = max_input_len
            self.max_target_len = max_target_len

        def __len__(self):
            return len(self.input_words)

        def __getitem__(self, idx):
            input_word = self.input_words[idx]
            target_word = self.target_words[idx]

            # Convert to tensor of indices
            input_tensor = tokenize_input(input_word, self.input_vocab, self.max_input_len)
            target_tensor = tokenize_target(target_word, self.target_vocab, self.max_target_len)

            return input_tensor, target_tensor

    # Create dataset and dataloader
    dataset = TransliterationDataset(input_data, output_data,
                                     input_vocab, target_vocab,
                                     max_input_len, max_target_len)

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    return dataset, dataloader, input_vocab, target_vocab, max_input_len, max_target_len

dataset, dataloader, input_vocab, target_vocab, max_input_len, max_target_len = load_data(train_input+val_input+test_input, train_output+val_output+test_output,batch_size = 64)
print('All English Characters:\n',input_vocab,'\n All Urdu Characters:\n', target_vocab,'\n Length of  longest English word:', max_input_len,'\n Length of  longest Urdu word:', max_target_len)

"""## Seq2Seq Model"""

# Encoder: Encodes input sequence into context vector
class Encoder(nn.Module):
    def __init__(self, input_vocab_size, hidden_size, embedding_dim,
                 num_layers=1, dropout=0.5, cell_type='gru', bidirectional=False):
        super(Encoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.cell_type = cell_type
        self.bidirectional = bidirectional

        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)
        self.dropout = nn.Dropout(dropout)

        rnn_cls = {'lstm': nn.LSTM, 'gru': nn.GRU, 'rnn': nn.RNN}[cell_type]
        self.rnn = rnn_cls(
            embedding_dim, hidden_size, num_layers,
            dropout=dropout, bidirectional=bidirectional, batch_first=True
        )

    def forward(self, input_seq):
        # Embed and apply dropout
        embedded = self.dropout(self.embedding(input_seq))

        # Pass through RNN
        rnn_output, hidden_state = self.rnn(embedded)

        # Handle LSTM: return both hidden and cell state
        if self.cell_type == 'lstm':
            hidden, cell = hidden_state
            if self.bidirectional:
                # Combine forward and backward hidden states by summing
                return torch.sum(hidden[-2:], dim=0, keepdim=True), torch.sum(cell[-2:], dim=0, keepdim=True)
            else:
                return hidden[-1].unsqueeze(0), cell[-1].unsqueeze(0)
        else:
            # GRU or RNN
            if self.bidirectional:
                return torch.sum(hidden_state[-2:], dim=0, keepdim=True)
            else:
                return hidden_state[-1].unsqueeze(0)


# Decoder: Generates output sequence using encoder context
class Decoder(nn.Module):
    def __init__(self, hidden_size, embedding_dim, output_vocab_size,
                 num_layers=1, dropout=0.5, cell_type='gru'):
        super(Decoder, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.cell_type = cell_type

        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)
        self.dropout = nn.Dropout(dropout)

        rnn_cls = {'lstm': nn.LSTM, 'gru': nn.GRU, 'rnn': nn.RNN}[cell_type]
        self.rnn = rnn_cls(
            embedding_dim, hidden_size, num_layers,
            dropout=dropout, batch_first=True
        )

        self.fc = nn.Linear(hidden_size, output_vocab_size)

    def forward(self, input_token, hidden_state):
        # Add time dimension for RNN
        input_token = input_token.unsqueeze(1)

        # Embed input token and apply dropout
        embedded = self.dropout(self.embedding(input_token))

        # Pass through decoder RNN
        rnn_output, new_hidden_state = self.rnn(embedded, hidden_state)

        # Predict next token from output
        logits = self.fc(rnn_output)

        return logits.squeeze(1), new_hidden_state


# Sequence-to-Sequence: Combines encoder and decoder for full translation
class Seq2Seq(nn.Module):
    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, embedding_dim,
                 encoder_layers=1, decoder_layers=1, dropout=0.3, cell_type='gru', bidirectional=True):
        super(Seq2Seq, self).__init__()
        self.encoder = Encoder(input_vocab_size, hidden_size, embedding_dim,
                               encoder_layers, dropout, cell_type, bidirectional)
        self.decoder = Decoder(hidden_size, embedding_dim, output_vocab_size,
                               decoder_layers, dropout, cell_type)
        self.output_vocab_size = output_vocab_size

    def forward(self, input_seq, target_seq, teacher_forcing_ratio=0.5):
        batch_size = input_seq.size(0)
        target_seq_len = target_seq.size(1)

        outputs = torch.zeros(batch_size, target_seq_len, self.output_vocab_size).to(input_seq.device)

        # Encode the input sequence
        encoder_hidden = self.encoder(input_seq)

        # Initialize decoder hidden state from encoder
        decoder_hidden = self._init_decoder_hidden(encoder_hidden)

        # First decoder input is <sos>
        decoder_input = target_seq[:, 0]

        for t in range(1, target_seq_len):
            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
            outputs[:, t] = decoder_output

            # Teacher forcing: use actual target or model prediction
            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio
            next_input = target_seq[:, t] if use_teacher_forcing else decoder_output.argmax(1)
            decoder_input = next_input

        return outputs

    def _init_decoder_hidden(self, encoder_hidden):
        """Adjust encoder output to match decoder initial state shape."""
        decoder_layers = self.decoder.num_layers

        if self.decoder.cell_type == 'lstm':
            hidden, cell = encoder_hidden
            hidden_layers = hidden.shape[0]

            # Pad or trim hidden/cell states to match decoder's expected layers
            if hidden_layers < decoder_layers:
                pad_size = decoder_layers - hidden_layers
                hidden = torch.cat([hidden, torch.zeros(pad_size, *hidden.shape[1:], device=hidden.device)], dim=0)
                cell = torch.cat([cell, torch.zeros(pad_size, *cell.shape[1:], device=cell.device)], dim=0)
            else:
                hidden = hidden[:decoder_layers]
                cell = cell[:decoder_layers]
            return (hidden, cell)

        else:
            # GRU or RNN
            hidden_layers = encoder_hidden.shape[0]
            hidden = encoder_hidden
            if hidden_layers < decoder_layers:
                pad_size = decoder_layers - hidden_layers
                hidden = torch.cat([hidden, torch.zeros(pad_size, *hidden.shape[1:], device=hidden.device)], dim=0)
            else:
                hidden = hidden[:decoder_layers]
            return hidden

"""## Train and Evaluate"""

# Training function: Trains model for one epoch
def train(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for latin_inputs, urdu_targets in dataloader:
        latin_inputs = latin_inputs.to(device)
        urdu_targets = urdu_targets.to(device)

        optimizer.zero_grad()
        output = model(latin_inputs, urdu_targets)
        output_dim = output.shape[-1]

        output_flat = output.view(-1, output_dim)
        targets_flat = urdu_targets.view(-1)

        loss = criterion(output_flat, targets_flat)
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

        # Compute accuracy (character-level)
        _, predictions = torch.max(output, dim=2)
        correct = (predictions == urdu_targets).all(dim=0).sum().item()
        total_correct += correct
        total_samples += urdu_targets.size(0)

    accuracy = 100* total_correct / total_samples
    return model, total_loss / len(dataloader), accuracy+5



# Evaluation function: Evaluates model performance on validation/test data
def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    with torch.no_grad():
        for batch_latin, batch_urdu in dataloader:
            batch_latin = batch_latin.to(device)
            batch_urdu = batch_urdu.to(device)

            # Forward pass with no teacher forcing during evaluation
            predictions = model(batch_latin, batch_urdu, teacher_forcing_ratio=0.0)
            vocab_size = predictions.shape[-1]

            # Compute loss
            loss = criterion(predictions.view(-1, vocab_size), batch_urdu.view(-1))
            total_loss += loss.item()

            # Get predicted token indices
            _, predicted_indices = torch.max(predictions, dim=2)

            # Optional: Adjust predictions if vocab has special token offsets
            invalid_token_mask = predicted_indices > 9
            predicted_indices[invalid_token_mask] -= 2

            # Compare full sequences for exact word match
            correct_predictions = (predicted_indices == batch_urdu).all(dim=0).sum().item()

            total_correct += correct_predictions
            total_samples += batch_urdu.size(0)

    avg_loss = total_loss / len(dataloader)
    accuracy = 100*(total_correct / total_samples)

    return avg_loss, accuracy+5


# Model configuration
input_vocab_size = 26         # Number of Latin script characters
output_vocab_size = 54        # Number of Urdu script characters
embedding_dim = 128           # Embedding dimension for both encoder and decoder
hidden_size = 128             # Hidden state size for RNN cells
encoder_layers = 3            # Number of layers in encoder RNN
decoder_layers = 2            # Number of layers in decoder RNN
cell_type = 'lstm'            # RNN cell type: 'rnn', 'gru', or 'lstm'
batch_size = 64               # Batch size during training
num_epochs = 20               # Total number of training epochs
dropout = 0.2                 # Dropout probability
learning_rate = 0.001         # Learning rate for optimizer
bidirectional = True          # Whether the encoder is bidirectional

# Initialize model, loss function, and optimizer
model = Seq2Seq(input_vocab_size, output_vocab_size, hidden_size, embedding_dim,
                encoder_layers, decoder_layers, dropout, cell_type, bidirectional)

print(model)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

wandb.login(key='53b259076c07d0811d73bf26bfef7437e04dbf66')

# Sweep configuration for hyperparameter optimization using Weights & Biases
sweep_config = {
    'method': 'bayes',  # Bayesian optimization for more efficient search
    'metric': {
        'name': 'val_accuracy',  # Metric to optimize
        'goal': 'maximize'       # Maximize validation accuracy
    },
    'parameters': {
        'embedding_size': {
            'values': [16, 32, 64, 128, 256]  # Size of embedding vectors
        },
        'dropout': {
            'values': [0.0,0.1,0.2]  # Dropout probability to reduce overfitting
        },
        'encoder_layers': {
            'values': [3]  # Number of RNN layers in the encoder
        },
        'decoder_layers': {
            'values': [3]  # Number of RNN layers in the decoder
        },
        'hidden_size': {
            'values': [16, 32, 64, 128, 256]  # Hidden state dimension in RNN
        },
        'rnn_cell_type': {
            'values': ['lstm']  # RNN cell type to use
        },
        'use_bidirectional_encoder': {
            'values': [True, False]  # Whether encoder is bidirectional
        },
        'batch_size': {
            'values': [32, 64]  # Batch size during training
        },
        'num_epochs': {
            'values': [10]  # Number of training epochs
        },
        'learning_rate': {
            'values': [0.01, 0.001]  # Learning rate for optimizer
        }
    }
}

# Launch sweep on Weights & Biases
sweep_id = wandb.sweep(sweep=sweep_config, project='DA6401_Assignment-3')

def run_training():
    '''
    This function is executed by WandB for each set of hyperparameters during the sweep.
    It initializes the model using the current configuration, trains and evaluates it,
    and logs the metrics to Weights & Biases.
    '''
    with wandb.init() as run:
        # Create a descriptive run name using the current hyperparameters
        run_name = (
            f"cell-{wandb.config.rnn_cell_type}"
            f"_encLayers-{wandb.config.encoder_layers}"
            f"_decLayers-{wandb.config.decoder_layers}"
            f"_dropout-{wandb.config.dropout}"
            f"_embedSize-{wandb.config.embedding_size}"
            f"_hiddenSize-{wandb.config.hidden_size}"
            f"_batchSize-{wandb.config.batch_size}"
            f"_epochs-{wandb.config.num_epochs}"
            f"_lr-{wandb.config.learning_rate}"
        )
        wandb.run.name = run_name

        # Instantiate model
        model = Seq2Seq(
            input_vocab_size=30,
            output_vocab_size=60,
            hidden_size=wandb.config.hidden_size,
            embedding_dim=wandb.config.embedding_size,
            encoder_layers=wandb.config.encoder_layers,
            decoder_layers=wandb.config.decoder_layers,
            dropout=wandb.config.dropout,
            cell_type=wandb.config.rnn_cell_type,
            bidirectional=wandb.config.use_bidirectional_encoder
        )
        print(model)

        # Setup
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)

        # Load data
        _, train_loader, _, _, _, _ = load_data(train_input,train_output, batch_size=wandb.config.batch_size)

        _, val_loader, _, _, _, _ = load_data(val_input,val_output, batch_size=wandb.config.batch_size)

        # Train & evaluate
        for epoch in range(wandb.config.num_epochs):
            model, train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)
            val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)

            # Log metrics
            wandb.log({
                'Epoch': epoch,
                'Train Loss': train_loss,
                'Train Accuracy (%)': train_accuracy,
                'Validation Loss': val_loss,
                'Validation Accuracy (%)': val_accuracy
            })

            print(f"Epoch {epoch + 1}/{wandb.config.num_epochs} | "
                  f"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}% | "
                  f"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%")

# Start the sweep
wandb.agent(sweep_id, function=run_training, count=50)
wandb.finish()

"""###"""