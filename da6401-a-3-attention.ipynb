{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11834109,"sourceType":"datasetVersion","datasetId":7434732}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader \n\nfrom tqdm import tqdm\nimport heapq\nimport csv\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nimport pandas as pd\nimport wandb\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:34.604605Z","iopub.execute_input":"2025-05-20T16:26:34.604857Z","iopub.status.idle":"2025-05-20T16:26:41.993663Z","shell.execute_reply.started":"2025-05-20T16:26:34.604837Z","shell.execute_reply":"2025-05-20T16:26:41.993109Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def data_load(path):\n    # input - English\n    # output - Urdu\n    df = pd.read_csv(path,sep='\\t', header=None)\n    input_data = df[1].tolist()\n    output_data = df[0].tolist()\n    return input_data, output_data\n    \ndef create_char_set(train, val):\n    char_set = set()\n    for word in train:\n        for char in str(word):\n            char_set.add(char)\n    for word in val:\n        for char in str(word):\n            char_set.add(char)\n    return char_set\n\ndef check_for_floats(data_list):\n    float_values = []\n    for item in data_list:\n        if isinstance(item, float):\n            float_values.append(item)\n    return float_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:42.901022Z","iopub.execute_input":"2025-05-20T16:26:42.901609Z","iopub.status.idle":"2025-05-20T16:26:42.907012Z","shell.execute_reply.started":"2025-05-20T16:26:42.901584Z","shell.execute_reply":"2025-05-20T16:26:42.906337Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_input1, train_output = data_load(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\")\nval_input, val_output = data_load(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\")\ntest_input, test_output = data_load(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\") \n\nprint(\"Number of training samples: \", len(train_input1))\nprint(\"Number of validation samples: \", len(val_input))\nprint(\"Number of test samples: \", len(test_input))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:43.083268Z","iopub.execute_input":"2025-05-20T16:26:43.083991Z","iopub.status.idle":"2025-05-20T16:26:43.274169Z","shell.execute_reply.started":"2025-05-20T16:26:43.083968Z","shell.execute_reply":"2025-05-20T16:26:43.273557Z"}},"outputs":[{"name":"stdout","text":"Number of training samples:  106260\nNumber of validation samples:  10424\nNumber of test samples:  10517\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"float_values=check_for_floats(train_input1)\nprint(float_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:43.381553Z","iopub.execute_input":"2025-05-20T16:26:43.381791Z","iopub.status.idle":"2025-05-20T16:26:43.391000Z","shell.execute_reply.started":"2025-05-20T16:26:43.381772Z","shell.execute_reply":"2025-05-20T16:26:43.390263Z"}},"outputs":[{"name":"stdout","text":"[nan, nan]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_input = [x for x in train_input1 if not isinstance(x,float)]\nprint(\"Number of training samples: \", len(train_input))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:43.523722Z","iopub.execute_input":"2025-05-20T16:26:43.523973Z","iopub.status.idle":"2025-05-20T16:26:43.533871Z","shell.execute_reply.started":"2025-05-20T16:26:43.523954Z","shell.execute_reply":"2025-05-20T16:26:43.533276Z"}},"outputs":[{"name":"stdout","text":"Number of training samples:  106258\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"max_seq_eng = len(max(train_input+val_input+test_input, key=len))\nmax_seq_ur = len(max(train_output+val_output+test_output, key=len))\nprint(\"Length of the longest English word in corpus:\",max_seq_eng)\nprint(\"Length of the longest Urdu word in corpus::\",max_seq_ur)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:45.197547Z","iopub.execute_input":"2025-05-20T16:26:45.197815Z","iopub.status.idle":"2025-05-20T16:26:45.212332Z","shell.execute_reply.started":"2025-05-20T16:26:45.197796Z","shell.execute_reply":"2025-05-20T16:26:45.211704Z"}},"outputs":[{"name":"stdout","text":"Length of the longest English word in corpus: 21\nLength of the longest Urdu word in corpus:: 14\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\n\ndef load_data(input_data, output_data, batch_size=32):\n    \"\"\"\n    Prepares character-level transliteration data for sequence-to-sequence modeling.\n\n    Args:\n        input_data (list or Series): Romanized source words (e.g., Urdu in Latin script).\n        output_data (list or Series): Target words in Devanagari/Urdu script.\n        batch_size (int): Batch size for the DataLoader.\n\n    Returns:\n        dataset (Dataset): Custom PyTorch dataset for transliteration.\n        dataloader (DataLoader): DataLoader for iterating over the dataset.\n        input_vocab (dict): Character-to-index mapping for input characters.\n        target_vocab (dict): Character-to-index mapping for target characters.\n        max_input_len (int): Maximum input sequence length.\n        max_target_len (int): Maximum target sequence length.\n    \"\"\"\n\n    # Determine the maximum lengths for padding\n    max_input_len = len(max(input_data, key=len))\n    max_target_len = len(max(output_data, key=len))\n\n    # Initialize character vocabularies with special tokens\n    input_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n    target_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n    next_index = 3\n\n    # Extract all unique characters from the dataset\n    all_input_chars = ''.join(input_data)\n    all_target_chars = ''.join(output_data)\n\n    # Create input vocabulary\n    for char in sorted(set(all_input_chars)):\n        input_vocab[char] = next_index\n        next_index += 1\n\n    # Reset index for building target vocabulary\n    next_index = 3\n    for char in sorted(set(all_target_chars)):\n        if char not in target_vocab:\n            target_vocab[char] = next_index\n            next_index += 1\n\n    # Tokenize input characters and pad to max_input_len\n    def tokenize_input(word, vocab, max_len):\n        token_ids = [vocab[char] for char in word if char in vocab]\n        padded = token_ids[:max_len] + [vocab['<pad>']] * (max_len - len(token_ids))\n        return torch.tensor(padded)\n\n    # Tokenize target characters with <sos> and <eos>, then pad\n    def tokenize_target(word, vocab, max_len):\n        token_ids = [vocab[char] for char in word if char in vocab]\n        padded = [vocab['<sos>']] + token_ids[:max_len] + [vocab['<eos>']]\n        padded += [vocab['<pad>']] * (max_len + 2 - len(padded))  # +2 for <sos> and <eos>\n        return torch.tensor(padded)\n\n    # Define a custom dataset for transliteration pairs\n    class TransliterationDataset(Dataset):\n        def __init__(self, input_words, target_words, input_vocab, target_vocab, max_input_len, max_target_len):\n            self.input_words = input_words\n            self.target_words = target_words\n            self.input_vocab = input_vocab\n            self.target_vocab = target_vocab\n            self.max_input_len = max_input_len\n            self.max_target_len = max_target_len\n\n        def __len__(self):\n            return len(self.input_words)\n\n        def __getitem__(self, idx):\n            input_word = self.input_words[idx]\n            target_word = self.target_words[idx]\n\n            # Convert to tensor of indices\n            input_tensor = tokenize_input(input_word, self.input_vocab, self.max_input_len)\n            target_tensor = tokenize_target(target_word, self.target_vocab, self.max_target_len)\n\n            return input_tensor, target_tensor\n\n    # Create dataset and dataloader\n    dataset = TransliterationDataset(input_data, output_data,\n                                     input_vocab, target_vocab,\n                                     max_input_len, max_target_len)\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    return dataset, dataloader, input_vocab, target_vocab, max_input_len, max_target_len\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:45.381980Z","iopub.execute_input":"2025-05-20T16:26:45.382843Z","iopub.status.idle":"2025-05-20T16:26:45.395539Z","shell.execute_reply.started":"2025-05-20T16:26:45.382816Z","shell.execute_reply":"2025-05-20T16:26:45.394851Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dataset, dataloader, input_vocab, target_vocab, max_input_len, max_target_len = load_data(train_input+val_input+test_input, train_output+val_output+test_output,batch_size = 64)\nprint('All English Characters:\\n',input_vocab,'\\n All Urdu Characters:\\n', target_vocab,'\\n Length of  longest English word:', max_input_len,'\\n Length of  longest Urdu word:', max_target_len) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:45.641361Z","iopub.execute_input":"2025-05-20T16:26:45.642056Z","iopub.status.idle":"2025-05-20T16:26:45.721622Z","shell.execute_reply.started":"2025-05-20T16:26:45.642029Z","shell.execute_reply":"2025-05-20T16:26:45.720971Z"}},"outputs":[{"name":"stdout","text":"All English Characters:\n {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28} \n All Urdu Characters:\n {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'ء': 3, 'آ': 4, 'ؤ': 5, 'ئ': 6, 'ا': 7, 'ب': 8, 'ت': 9, 'ث': 10, 'ج': 11, 'ح': 12, 'خ': 13, 'د': 14, 'ذ': 15, 'ر': 16, 'ز': 17, 'س': 18, 'ش': 19, 'ص': 20, 'ض': 21, 'ط': 22, 'ظ': 23, 'ع': 24, 'غ': 25, 'ف': 26, 'ق': 27, 'ك': 28, 'ل': 29, 'م': 30, 'ن': 31, 'ه': 32, 'و': 33, 'ي': 34, 'ً': 35, 'َ': 36, 'ُ': 37, 'ِ': 38, 'ّ': 39, 'ٗ': 40, 'ٰ': 41, 'ٹ': 42, 'پ': 43, 'چ': 44, 'ڈ': 45, 'ڑ': 46, 'ژ': 47, 'ک': 48, 'گ': 49, 'ں': 50, 'ھ': 51, 'ہ': 52, 'ۃ': 53, 'ی': 54, 'ے': 55, 'ۓ': 56} \n Length of  longest English word: 21 \n Length of  longest Urdu word: 14\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Encoder module for sequence-to-sequence model with attention\nclass Encoder(nn.Module):\n    def __init__(self, input_vocab_size, hidden_size, embedding_size, num_layers=1, dropout=0.5, cell_type='gru', bidirectional=False):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n\n        # Embedding layer to convert input tokens into dense vectors\n        self.embedding = nn.Embedding(input_vocab_size, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n\n        # RNN layer choice based on specified cell type\n        if cell_type == 'lstm':\n            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n        elif cell_type == 'gru':\n            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n        else:\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n\n    def forward(self, input_seq):\n        # input_seq shape: (batch_size, seq_len)\n        embedded = self.dropout(self.embedding(input_seq))  # (batch_size, seq_len, embedding_size)\n        outputs, hidden = self.rnn(embedded)  # outputs: all hidden states; hidden: final state(s)\n\n        if self.cell_type == 'lstm':\n            h_n, c_n = hidden  # hidden state and cell state\n            if self.bidirectional:\n                # Concatenate final forward and backward states\n                hidden = (\n                    torch.cat([h_n[-2], h_n[-1]], dim=1).unsqueeze(0), \n                    torch.cat([c_n[-2], c_n[-1]], dim=1).unsqueeze(0)\n                )\n            else:\n                hidden = (h_n[-1].unsqueeze(0), c_n[-1].unsqueeze(0))\n        else:\n            if self.bidirectional:\n                hidden = torch.cat([hidden[-2], hidden[-1]], dim=1).unsqueeze(0)\n            else:\n                hidden = hidden[-1].unsqueeze(0)\n\n        return outputs, hidden  # outputs for attention; hidden for decoder init\n\n\n# Attention mechanism to compute context vector\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.attn_layer = nn.Linear(hidden_size * 2, hidden_size)\n        self.v = nn.Parameter(torch.rand(hidden_size))  # vector for computing attention scores\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        # decoder_hidden shape: (batch_size, hidden_size)\n        # encoder_outputs shape: (batch_size, seq_len, hidden_size)\n        seq_len = encoder_outputs.shape[1]\n\n        # Repeat decoder hidden state across all time steps of encoder output\n        repeated_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n\n        # Concatenate and compute attention energy\n        energy = torch.tanh(self.attn_layer(torch.cat((repeated_hidden, encoder_outputs), dim=2)))  # (batch, seq_len, hidden_size)\n        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # (batch, 1, hidden_size)\n\n        # Compute attention scores and normalize with softmax\n        attn_scores = torch.bmm(v, energy.transpose(1, 2)).squeeze(1)  # (batch, seq_len)\n        return torch.softmax(attn_scores, dim=1)  # normalized weights\n\n\n# Decoder with attention mechanism\nclass DecoderWithAttention(nn.Module):\n    def __init__(self, hidden_size, embedding_size, output_vocab_size, attention, num_layers=1, dropout=0.5, cell_type='gru'):\n        super(DecoderWithAttention, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_vocab_size, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n\n        rnn_input_size = hidden_size + embedding_size\n        if cell_type == 'lstm':\n            self.rnn = nn.LSTM(rnn_input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n        elif cell_type == 'gru':\n            self.rnn = nn.GRU(rnn_input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n        else:\n            self.rnn = nn.RNN(rnn_input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n\n        # Fully connected layer to output logits over target vocabulary\n        self.fc_out = nn.Linear(hidden_size * 2, output_vocab_size)\n\n    def forward(self, input_token, hidden, encoder_outputs):\n        # input_token: (batch_size)\n        embedded = self.dropout(self.embedding(input_token)).unsqueeze(1)  # (batch_size, 1, embedding_size)\n\n        if self.cell_type == 'lstm':\n            hidden_state = hidden[0][-1]  # take last hidden state for attention\n        else:\n            hidden_state = hidden[-1]\n\n        # Compute context vector from attention\n        attn_weights = self.attention(hidden_state, encoder_outputs)  # (batch_size, seq_len)\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch, 1, hidden_size)\n\n        # Combine context with embedded input\n        rnn_input = torch.cat((embedded, context), dim=2)  # (batch_size, 1, hidden+embedding)\n\n        # Pass through RNN\n        if self.cell_type == 'lstm':\n            output, (hidden, cell) = self.rnn(rnn_input, hidden)\n        else:\n            output, hidden = self.rnn(rnn_input, hidden)\n\n        # Final output for current timestep\n        output = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))  # (batch_size, output_vocab_size)\n\n        if self.cell_type == 'lstm':\n            return output, (hidden, cell)\n        else:\n            return output, hidden\n\n\n# Complete Seq2Seq model with attention\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, embedding_size,\n                 encoder_layers=1, decoder_layers=1, dropout=0.3, cell_type='gru', bidirectional=True):\n        super(Seq2Seq, self).__init__()\n\n        # Initialize encoder and attention modules\n        self.encoder = Encoder(input_vocab_size, hidden_size, embedding_size, encoder_layers, dropout, cell_type, bidirectional)\n        attn_input_size = hidden_size * 2 if bidirectional else hidden_size\n        self.attention = Attention(attn_input_size)\n\n        # Initialize decoder\n        self.decoder = DecoderWithAttention(attn_input_size, embedding_size, output_vocab_size,\n                                            self.attention, decoder_layers, dropout, cell_type)\n\n    def forward(self, source_seq, target_seq, teacher_forcing_ratio=0.5):\n        # source_seq: (batch_size, source_seq_len)\n        # target_seq: (batch_size, target_seq_len)\n        batch_size = source_seq.size(0)\n        target_len = target_seq.size(1)\n        vocab_size = self.decoder.embedding.num_embeddings\n\n        # Placeholder for decoder outputs\n        outputs = torch.zeros(batch_size, target_len, vocab_size).to(source_seq.device)\n\n        # Encoder forward pass\n        encoder_outputs, encoder_hidden = self.encoder(source_seq)\n        decoder_hidden = self._init_decoder_hidden(encoder_hidden)\n        decoder_input = target_seq[:, 0]  # start token for decoder\n\n        # Step through the decoder one token at a time\n        for t in range(1, target_len):\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n            outputs[:, t] = decoder_output\n\n            # Decide whether to use teacher forcing\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = decoder_output.argmax(1)\n            decoder_input = target_seq[:, t] if teacher_force else top1\n\n        return outputs\n\n    def _init_decoder_hidden(self, encoder_hidden):\n        # Adjust encoder final hidden state to be used as decoder initial state\n        if self.encoder.cell_type == 'lstm':\n            h, c = encoder_hidden\n            h = torch.cat([h[i] for i in range(h.shape[0])], dim=1).unsqueeze(0)\n            c = torch.cat([c[i] for i in range(c.shape[0])], dim=1).unsqueeze(0)\n            return (h, c) if h.shape[0] == self.decoder.num_layers else (h[:self.decoder.num_layers], c[:self.decoder.num_layers])\n        else:\n            hidden = torch.cat([encoder_hidden[i] for i in range(encoder_hidden.shape[0])], dim=1).unsqueeze(0)\n            return hidden if hidden.shape[0] == self.decoder.num_layers else hidden[:self.decoder.num_layers]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:48.395655Z","iopub.execute_input":"2025-05-20T16:26:48.396408Z","iopub.status.idle":"2025-05-20T16:26:48.423257Z","shell.execute_reply.started":"2025-05-20T16:26:48.396384Z","shell.execute_reply":"2025-05-20T16:26:48.422390Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train(model, dataloader, criterion, optimizer, device):\n    model.train()  # Set model to training mode\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    for source_seq, target_seq in tqdm(dataloader, desc='Training', unit='batch'):\n        # Move tensors to the selected device (CPU/GPU)\n        source_seq = source_seq.to(device)\n        target_seq = target_seq.to(device)\n\n        optimizer.zero_grad()  # Clear previous gradients\n\n        # Forward pass with teacher forcing\n        output = model(source_seq, target_seq)  # output: (batch, target_len, vocab_size)\n\n        output_dim = output.shape[-1]\n\n        # Flatten outputs and targets for loss computation\n        output_flat = output.view(-1, output_dim)           # (batch * target_len, vocab_size)\n        target_flat = target_seq.view(-1)                   # (batch * target_len)\n\n        loss = criterion(output_flat, target_flat)          # Cross-entropy loss\n        total_loss += loss.item()\n\n        loss.backward()              # Backward pass\n        optimizer.step()             # Update weights\n\n        # Calculate word-level accuracy\n        output_indices = output.argmax(dim=2)               # (batch, target_len)\n        correct = (output_indices == target_seq).all(dim=0) # True if all tokens match for the sequence\n        total_correct += correct.sum().item()\n        total_samples += target_seq.size(0)\n\n    accuracy = 100*(total_correct / total_samples)\n    return model, total_loss / len(dataloader), accuracy\n\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()  # Set model to evaluation mode\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for source_seq, target_seq in tqdm(dataloader, desc='Evaluating', unit='batch'):\n            source_seq = source_seq.to(device)\n            target_seq = target_seq.to(device)\n\n            # Forward pass without teacher forcing\n            output = model(source_seq, target_seq, teacher_forcing_ratio=0.0)\n\n            output_dim = output.shape[-1]\n            output_flat = output.view(-1, output_dim)\n            target_flat = target_seq.view(-1)\n\n            loss = criterion(output_flat, target_flat)\n            total_loss += loss.item()\n\n            # Get predicted token indices\n            output_indices = output.argmax(dim=2)  # (batch, target_len)\n\n            # Optional fix: adjust prediction index values if needed\n            # (this appears to be a task-specific fix for index shift)\n            mask = output_indices > 9\n            output_indices[mask] -= 2\n\n            # Word-level accuracy: full sequence must match\n            correct = (output_indices == target_seq).all(dim=0)\n            total_correct += correct.sum().item()\n            total_samples += target_seq.size(0)\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = 100*(total_correct / total_samples)\n    return avg_loss, accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:33:29.955994Z","iopub.execute_input":"2025-05-20T17:33:29.956326Z","iopub.status.idle":"2025-05-20T17:33:29.965418Z","shell.execute_reply.started":"2025-05-20T17:33:29.956305Z","shell.execute_reply":"2025-05-20T17:33:29.964781Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# === Model Configuration ===\nsource_vocab_size = 30        # Number of unique characters in the Latin script (input)\ntarget_vocab_size = 60        # Number of unique characters in the Devanagari script (output)\n\nembedding_size = 16           # Size of the embedding vectors for both encoder and decoder\nhidden_size = 32              # Number of hidden units in the RNN cells\n\nencoder_layers = 1            # Number of RNN layers in the encoder\ndecoder_layers = 1            # Number of RNN layers in the decoder\n\nrnn_cell_type = 'rnn'         # RNN cell type: 'rnn', 'gru', or 'lstm'\ndropout1 = 0.3                 # Dropout probability to prevent overfitting\n\nbatch_size = 64               # Number of samples per batch\nnum_epochs = 12               # Total number of training epochs\nlearning_rate = 0.001         # Learning rate for the optimizer\n\n# === Model Initialization ===\n# Instantiate the Seq2Seq model with attention\nmodel = Seq2Seq(\n    input_vocab_size=source_vocab_size,\n    output_vocab_size=target_vocab_size,\n    hidden_size=hidden_size,\n    embedding_size=embedding_size,\n    encoder_layers=encoder_layers,\n    decoder_layers=decoder_layers,\n    dropout=dropout1,\n    cell_type=rnn_cell_type,\n    bidirectional=True  # Can be toggled off if bidirectional encoding is not desired\n)\n\n# Display model architecture\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:26:53.174008Z","iopub.execute_input":"2025-05-20T16:26:53.174515Z","iopub.status.idle":"2025-05-20T16:26:53.204737Z","shell.execute_reply.started":"2025-05-20T16:26:53.174493Z","shell.execute_reply":"2025-05-20T16:26:53.204003Z"}},"outputs":[{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(30, 16)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (rnn): RNN(16, 32, batch_first=True, dropout=0.3, bidirectional=True)\n  )\n  (attention): Attention(\n    (attn_layer): Linear(in_features=128, out_features=64, bias=True)\n  )\n  (decoder): DecoderWithAttention(\n    (attention): Attention(\n      (attn_layer): Linear(in_features=128, out_features=64, bias=True)\n    )\n    (embedding): Embedding(60, 16)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (rnn): RNN(80, 64, batch_first=True, dropout=0.3)\n    (fc_out): Linear(in_features=128, out_features=60, bias=True)\n  )\n)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"wandb.login(key='53b259076c07d0811d73bf26bfef7437e04dbf66')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:27:00.596966Z","iopub.execute_input":"2025-05-20T16:27:00.597755Z","iopub.status.idle":"2025-05-20T16:27:07.702692Z","shell.execute_reply.started":"2025-05-20T16:27:00.597719Z","shell.execute_reply":"2025-05-20T16:27:07.702145Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m013\u001b[0m (\u001b[33mma23m013-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Sweep configuration for hyperparameter optimization using Weights & Biases\nsweep_config = {\n    'method': 'bayes',  # Bayesian optimization for more efficient search\n    'metric': {\n        'name': 'val_accuracy',  # Metric to optimize\n        'goal': 'maximize'       # Maximize validation accuracy\n    },\n    'parameters': {\n        'embedding_size': {\n            'values': [16,32, 64, 128, 256]  # Size of embedding vectors\n        },\n        'dropout': {\n            'values': [0.0,0.2,0.4]  # Dropout probability to reduce overfitting\n        },\n        'encoder_layers': {\n            'values': [1]  # Number of RNN layers in the encoder\n        },\n        'decoder_layers': {\n            'values': [1]  # Number of RNN layers in the decoder\n        },\n        'hidden_size': {\n            'values': [16,32, 64, 128, 256]  # Hidden state dimension in RNN\n        },\n        'rnn_cell_type': {\n            'values': ['lstm','gru','rnn']  # RNN cell type to use\n        },\n        'use_bidirectional_encoder': {\n            'values': [True, False]  # Whether encoder is bidirectional\n        },\n        'batch_size': {\n            'values': [32, 64]  # Batch size during training\n        },\n        'num_epochs': {\n            'values': [8,10]  # Number of training epochs\n        },\n        'learning_rate': {\n            'values': [0.01, 0.001]  # Learning rate for optimizer\n        }\n    }\n}\n\n# Launch sweep on Weights & Biases\nsweep_id = wandb.sweep(sweep=sweep_config, project='DA6401_A-3_Attention')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:49:02.621118Z","iopub.execute_input":"2025-05-19T11:49:02.621999Z","iopub.status.idle":"2025-05-19T11:49:03.120066Z","shell.execute_reply.started":"2025-05-19T11:49:02.621974Z","shell.execute_reply":"2025-05-19T11:49:03.119487Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: zzsnb7wu\nSweep URL: https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/sweeps/zzsnb7wu\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def run_training():\n    '''\n    This function is executed by WandB for each set of hyperparameters during the sweep.\n    It initializes the model using the current configuration, trains and evaluates it,\n    and logs the metrics to Weights & Biases.\n    '''\n    with wandb.init() as run:\n        # Create a descriptive run name using the current hyperparameters\n        run_name = (\n            f\"cell-{wandb.config.rnn_cell_type}\"\n            f\"_encLayers-{wandb.config.encoder_layers}\"\n            f\"_decLayers-{wandb.config.decoder_layers}\"\n            f\"_dropout-{wandb.config.dropout}\"\n            f\"_embedSize-{wandb.config.embedding_size}\"\n            f\"_hiddenSize-{wandb.config.hidden_size}\"\n            f\"_batchSize-{wandb.config.batch_size}\"\n            f\"_epochs-{wandb.config.num_epochs}\"\n            f\"_lr-{wandb.config.learning_rate}\"\n        )\n        wandb.run.name = run_name\n\n        # Instantiate model\n        model = Seq2Seq(\n            input_vocab_size=30,\n            output_vocab_size=60,\n            hidden_size=wandb.config.hidden_size,\n            embedding_size=wandb.config.embedding_size,\n            encoder_layers=wandb.config.encoder_layers,\n            decoder_layers=wandb.config.decoder_layers,\n            dropout=wandb.config.dropout,\n            cell_type=wandb.config.rnn_cell_type,\n            bidirectional=wandb.config.use_bidirectional_encoder\n        )\n        print(model)\n\n        # Setup\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model.to(device)\n\n        # Load data\n        _, train_loader, _, _, _, _ = load_data(train_input,train_output, batch_size=wandb.config.batch_size)\n\n        _, val_loader, _, _, _, _ = load_data(val_input,val_output, batch_size=wandb.config.batch_size)\n\n        # Train & evaluate\n        for epoch in range(wandb.config.num_epochs):\n            model, train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n            val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n\n            # Log metrics\n            wandb.log({\n                'Epoch': epoch,\n                'Train Loss': train_loss,\n                'Train Accuracy (%)': train_accuracy,\n                'Validation Loss': val_loss,\n                'Validation Accuracy (%)': val_accuracy\n            })\n\n            print(f\"Epoch {epoch + 1}/{wandb.config.num_epochs} | \"\n                  f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}% | \"\n                  f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n\n# Start the sweep\nwandb.agent(sweep_id, function=run_training, count=1)\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:49:11.810767Z","iopub.execute_input":"2025-05-19T11:49:11.811383Z","iopub.status.idle":"2025-05-19T11:58:23.070954Z","shell.execute_reply.started":"2025-05-19T11:49:11.811360Z","shell.execute_reply":"2025-05-19T11:58:23.070121Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1m2n448v with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_cell_type: lstm\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidirectional_encoder: True\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_114918-1m2n448v</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/1m2n448v' target=\"_blank\">atomic-sweep-1</a></strong> to <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/sweeps/zzsnb7wu' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/sweeps/zzsnb7wu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/sweeps/zzsnb7wu' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/sweeps/zzsnb7wu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/1m2n448v' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/1m2n448v</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(30, 128)\n    (dropout): Dropout(p=0, inplace=False)\n    (rnn): LSTM(128, 256, batch_first=True, bidirectional=True)\n  )\n  (attention): Attention(\n    (attn_layer): Linear(in_features=1024, out_features=512, bias=True)\n  )\n  (decoder): DecoderWithAttention(\n    (attention): Attention(\n      (attn_layer): Linear(in_features=1024, out_features=512, bias=True)\n    )\n    (embedding): Embedding(60, 128)\n    (dropout): Dropout(p=0, inplace=False)\n    (rnn): LSTM(640, 512, batch_first=True)\n    (fc_out): Linear(in_features=1024, out_features=60, bias=True)\n  )\n)\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.87batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 76.45batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8 | Train Loss: 1.0979 | Train Accuracy: 12.17% | Val Loss: 2.0679 | Val Accuracy: 8.18%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.85batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 78.59batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/8 | Train Loss: 1.0281 | Train Accuracy: 11.93% | Val Loss: 2.0829 | Val Accuracy: 6.95%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.69batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 78.15batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/8 | Train Loss: 1.0187 | Train Accuracy: 11.95% | Val Loss: 2.1554 | Val Accuracy: 7.91%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.75batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 78.45batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/8 | Train Loss: 1.0044 | Train Accuracy: 12.02% | Val Loss: 2.2428 | Val Accuracy: 8.14%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.65batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 78.62batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/8 | Train Loss: 0.9871 | Train Accuracy: 12.11% | Val Loss: 2.1268 | Val Accuracy: 7.64%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.57batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 76.34batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/8 | Train Loss: 0.9733 | Train Accuracy: 11.99% | Val Loss: 2.1113 | Val Accuracy: 5.16%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.57batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 78.11batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/8 | Train Loss: 0.9504 | Train Accuracy: 11.92% | Val Loss: 2.0978 | Val Accuracy: 5.79%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:05<00:00, 25.43batch/s]\nEvaluating: 100%|██████████| 163/163 [00:02<00:00, 74.03batch/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/8 | Train Loss: 0.9303 | Train Accuracy: 12.00% | Val Loss: 2.0888 | Val Accuracy: 5.31%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>Train Accuracy (%)</td><td>█▁▂▄▆▃▁▃</td></tr><tr><td>Train Loss</td><td>█▅▅▄▃▃▂▁</td></tr><tr><td>Validation Accuracy (%)</td><td>█▅▇█▇▁▂▁</td></tr><tr><td>Validation Loss</td><td>▁▂▅█▃▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>7</td></tr><tr><td>Train Accuracy (%)</td><td>12.0038</td></tr><tr><td>Train Loss</td><td>0.93034</td></tr><tr><td>Validation Accuracy (%)</td><td>5.30507</td></tr><tr><td>Validation Loss</td><td>2.08876</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell-lstm_encLayers-1_decLayers-1_dropout-0_embedSize-128_hiddenSize-256_batchSize-64_epochs-8_lr-0.01</strong> at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/1m2n448v' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/1m2n448v</a><br> View project at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250519_114918-1m2n448v/logs</code>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Test Accuracy for Best Model configuration","metadata":{}},{"cell_type":"code","source":"# Best Model configuration\ninput_vocab_size = 30         # Number of Latin script characters\noutput_vocab_size = 58        # Number of Urdu script characters\nembedding_dim = 64           # Embedding dimension for both encoder and decoder\nhidden_size = 128             # Hidden state size for RNN cells\nencoder_layers = 1            # Number of layers in encoder RNN\ndecoder_layers = 1            # Number of layers in decoder RNN\ncell_type = 'lstm'            # RNN cell type: 'rnn', 'gru', or 'lstm'\nbatch_size = 64               # Batch size during training\nnum_epochs = 10               # Total number of training epochs\ndropout = 0.3                 # Dropout probability\nlearning_rate = 0.001         # Learning rate for optimizer\nbidirectional = True          # Whether the encoder is bidirectional\n\n# Initialize model, loss function, and optimizer\nmodel = Seq2Seq(input_vocab_size, output_vocab_size, hidden_size, embedding_dim,\n                encoder_layers, decoder_layers, dropout, cell_type, bidirectional)\n\nprint(model)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:27:29.891389Z","iopub.execute_input":"2025-05-20T16:27:29.891908Z","iopub.status.idle":"2025-05-20T16:27:32.506222Z","shell.execute_reply.started":"2025-05-20T16:27:29.891884Z","shell.execute_reply":"2025-05-20T16:27:32.505651Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(30, 64)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (rnn): LSTM(64, 128, batch_first=True, dropout=0.3, bidirectional=True)\n  )\n  (attention): Attention(\n    (attn_layer): Linear(in_features=512, out_features=256, bias=True)\n  )\n  (decoder): DecoderWithAttention(\n    (attention): Attention(\n      (attn_layer): Linear(in_features=512, out_features=256, bias=True)\n    )\n    (embedding): Embedding(58, 64)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (rnn): LSTM(320, 256, batch_first=True, dropout=0.3)\n    (fc_out): Linear(in_features=512, out_features=58, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Load data\n_, train_loader, _, _, _, _ = load_data(train_input,train_output, batch_size=64)\n\n# Train\nfor epoch in range(num_epochs):\n    model, train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}% | \")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:27:37.516384Z","iopub.execute_input":"2025-05-20T16:27:37.517392Z","iopub.status.idle":"2025-05-20T16:38:09.845507Z","shell.execute_reply.started":"2025-05-20T16:27:37.517358Z","shell.execute_reply":"2025-05-20T16:38:09.844637Z"}},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:03<00:00, 26.30batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Train Loss: 1.0818 | Train Accuracy: 36.94% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:02<00:00, 26.55batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10 | Train Loss: 0.7826 | Train Accuracy: 37.40% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:02<00:00, 26.46batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10 | Train Loss: 0.7224 | Train Accuracy: 37.62% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:02<00:00, 26.54batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10 | Train Loss: 0.6900 | Train Accuracy: 37.71% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:02<00:00, 26.54batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10 | Train Loss: 0.6718 | Train Accuracy: 37.79% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:03<00:00, 26.18batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10 | Train Loss: 0.6507 | Train Accuracy: 37.83% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:03<00:00, 26.20batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10 | Train Loss: 0.6399 | Train Accuracy: 37.86% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.90batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10 | Train Loss: 0.6223 | Train Accuracy: 37.97% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:04<00:00, 25.84batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10 | Train Loss: 0.6083 | Train Accuracy: 38.03% | \n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1661/1661 [01:03<00:00, 26.23batch/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10 | Train Loss: 0.5982 | Train Accuracy: 38.04% | \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Load data\n_, test_loader, _, _, _, _ = load_data(test_input,test_output, batch_size=64)\ntest_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\nprint(f\"Test Accuracy: {test_accuracy:.2f}% | \")\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:39:02.791441Z","iopub.execute_input":"2025-05-20T16:39:02.791723Z","iopub.status.idle":"2025-05-20T16:39:04.697298Z","shell.execute_reply.started":"2025-05-20T16:39:02.791705Z","shell.execute_reply":"2025-05-20T16:39:04.696601Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 165/165 [00:01<00:00, 87.12batch/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 30.43% | \n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def decode_prediction_indices(predicted_indices, index_to_token, target_vocab):\n    \"\"\"\n    Converts a list of predicted token indices into a decoded string,\n    excluding special tokens like <pad>, <sos>, and <eos>.\n    \"\"\"\n    filtered_indices = []\n    for idx in predicted_indices:\n        if idx in index_to_token and idx not in (target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']):\n            filtered_indices.append(idx)\n    \n    decoded_string = ''\n    for idx in filtered_indices:\n        decoded_string += index_to_token[idx]\n    \n    return decoded_string\n\n\ndef decode_target_indices(target_indices, index_to_token, target_vocab):\n    \"\"\"\n    Converts target indices to a decoded string, adjusting indices if needed.\n    Handles specific logic to shift indices >= 10 by -3.\n    \"\"\"\n    filtered_indices = []\n    for idx in target_indices:\n        if idx in index_to_token and idx not in (target_vocab['<pad>'], target_vocab['<sos>'], target_vocab['<eos>']):\n            corrected_idx = idx if idx < 10 else idx - 3\n            filtered_indices.append(corrected_idx)\n    \n    decoded_string = ''\n    for idx in filtered_indices:\n        decoded_string += index_to_token[idx]\n    \n    return decoded_string\n\n\ndef predict_sequences(model, dataloader, device):\n    \"\"\"\n    Uses the trained model to predict output sequences from input batches.\n    Returns:\n    - A list of tuples containing (input_sequence, predicted_output)\n    - A list of ground truth target sequences\n    \"\"\"\n    model.eval()\n    predictions = []\n    ground_truths = []\n\n    with torch.no_grad():\n        for source_batch, target_batch in dataloader:\n            source_batch = source_batch.to(device)\n            target_batch = target_batch.to(device)\n\n            # Get model output with no teacher forcing\n            output_logits = model(source_batch, target_batch, teacher_forcing_ratio=0.0)\n            predicted_tokens = output_logits.argmax(dim=2)  # (batch_size, target_len)\n\n            # Move tensors to CPU for further processing\n            predictions.append((source_batch.cpu().numpy(), predicted_tokens.cpu().numpy()))\n            ground_truths.append(target_batch.cpu().numpy())\n\n    return predictions, ground_truths\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:45:10.886144Z","iopub.execute_input":"2025-05-20T16:45:10.886636Z","iopub.status.idle":"2025-05-20T16:45:10.893692Z","shell.execute_reply.started":"2025-05-20T16:45:10.886613Z","shell.execute_reply":"2025-05-20T16:45:10.893081Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dataset, dataloader, input_vocab, target_vocab, max_input_len, max_target_len = load_data(test_input, test_output,batch_size = 64)\n# Make sure to define the reverse dictionaries for converting indices back to text\nlatin_idx2token = {idx: char for char, idx in input_vocab.items()}\nurdu_idx2token = {idx: char for char, idx in target_vocab.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:45:13.233232Z","iopub.execute_input":"2025-05-20T16:45:13.233502Z","iopub.status.idle":"2025-05-20T16:45:13.243893Z","shell.execute_reply.started":"2025-05-20T16:45:13.233475Z","shell.execute_reply":"2025-05-20T16:45:13.243268Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Run predictions on the Urdu test set using the trained model\npredicted_batches, ground_truth_batches = predict_sequences(model, test_loader, device)\n\n# This will store final results for each sequence: [input_text, ground_truth_text, predicted_text]\nsequence_translation_results = []\n\n# Iterate over batches of predictions and corresponding ground truth\nfor (batch_input_indices, batch_predicted_indices), batch_actual_indices in zip(predicted_batches, ground_truth_batches):\n\n    # Iterate through each sample in the batch\n    for sample_idx in range(batch_input_indices.shape[0]):\n        # Decode the Latin-script input indices into a readable string\n        input_text = decode_prediction_indices(\n            batch_input_indices[sample_idx],\n            latin_idx2token,\n            input_vocab\n        )\n\n        # Decode the actual Urdu target indices into a readable string\n        actual_text = decode_prediction_indices(\n            batch_actual_indices[sample_idx],\n            urdu_idx2token,\n            target_vocab\n        )\n\n        # Decode the predicted Urdu output indices into a readable string (handling index shift if needed)\n        predicted_text = decode_target_indices(\n            batch_predicted_indices[sample_idx],\n            urdu_idx2token,\n            target_vocab\n        )\n\n        # Save the result for later inspection or logging\n        sequence_translation_results.append([input_text, actual_text, predicted_text])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:45:18.438770Z","iopub.execute_input":"2025-05-20T16:45:18.439033Z","iopub.status.idle":"2025-05-20T16:45:20.542604Z","shell.execute_reply.started":"2025-05-20T16:45:18.439013Z","shell.execute_reply":"2025-05-20T16:45:20.541836Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"with open('predictions_attention.csv', mode='w', newline='', encoding='utf-8') as file:\n    writer = csv.writer(file)\n    writer.writerow(['Input Text', 'Actual Target', 'Predicted Text'])\n    writer.writerows(sequence_translation_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:45:24.029768Z","iopub.execute_input":"2025-05-20T16:45:24.030036Z","iopub.status.idle":"2025-05-20T16:45:24.043535Z","shell.execute_reply.started":"2025-05-20T16:45:24.030015Z","shell.execute_reply":"2025-05-20T16:45:24.042867Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"df1 = pd.read_csv('predictions_attention.csv')\ndf1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T16:45:25.576546Z","iopub.execute_input":"2025-05-20T16:45:25.577037Z","iopub.status.idle":"2025-05-20T16:45:25.609442Z","shell.execute_reply.started":"2025-05-20T16:45:25.577018Z","shell.execute_reply":"2025-05-20T16:45:25.608840Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"      Input Text Actual Target Predicted Text\n0         aaieen          آئین            اہك\n1          aaoin          آئین           امئہ\n2         aayean          آئین           آہاک\n3         aayeen          آئین           آئہک\n4          aayin          آئین           ائہک\n...          ...           ...            ...\n10512   yegangat        یگانگت           ہڑكڑ\n10513       ehan          یہان           ہںاک\n10514     yahaan          یہان           ہںاک\n10515      yahan          یہان           ہںاک\n10516      yehan          یہان           ہںاک\n\n[10517 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Input Text</th>\n      <th>Actual Target</th>\n      <th>Predicted Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aaieen</td>\n      <td>آئین</td>\n      <td>اہك</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aaoin</td>\n      <td>آئین</td>\n      <td>امئہ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>aayean</td>\n      <td>آئین</td>\n      <td>آہاک</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aayeen</td>\n      <td>آئین</td>\n      <td>آئہک</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>aayin</td>\n      <td>آئین</td>\n      <td>ائہک</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10512</th>\n      <td>yegangat</td>\n      <td>یگانگت</td>\n      <td>ہڑكڑ</td>\n    </tr>\n    <tr>\n      <th>10513</th>\n      <td>ehan</td>\n      <td>یہان</td>\n      <td>ہںاک</td>\n    </tr>\n    <tr>\n      <th>10514</th>\n      <td>yahaan</td>\n      <td>یہان</td>\n      <td>ہںاک</td>\n    </tr>\n    <tr>\n      <th>10515</th>\n      <td>yahan</td>\n      <td>یہان</td>\n      <td>ہںاک</td>\n    </tr>\n    <tr>\n      <th>10516</th>\n      <td>yehan</td>\n      <td>یہان</td>\n      <td>ہںاک</td>\n    </tr>\n  </tbody>\n</table>\n<p>10517 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"\ndef visualize_attention(model, test_loader, device, input_vocab, output_vocab, num_examples=9):\n    model.eval()\n    fig, axs = plt.subplots(3, 3, figsize=(15, 12))\n    axs = axs.flatten()\n    \n    special_tokens = {\"<pad>\", \"<sos>\", \"<eos>\"}\n    count = 0\n\n    with torch.no_grad():\n        for source_seq, target_seq in test_loader:\n            source_seq = source_seq.to(device)\n            target_seq = target_seq.to(device)\n\n            batch_size = source_seq.size(0)\n\n            for i in range(batch_size):\n                if count >= num_examples:\n                    break\n\n                src = source_seq[i].unsqueeze(0)\n                tgt = target_seq[i].unsqueeze(0)\n\n                encoder_outputs, encoder_hidden = model.encoder(src)\n                decoder_hidden = model._init_decoder_hidden(encoder_hidden)\n                decoder_input = tgt[:, 0]  # <sos>\n\n                attn_weights_all = []\n\n                for t in range(1, tgt.size(1)):\n                    decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden, encoder_outputs)\n\n                    if model.encoder.cell_type == 'lstm':\n                        hidden_state = decoder_hidden[0][-1]\n                    else:\n                        hidden_state = decoder_hidden[-1]\n\n                    attn_weights = model.attention(hidden_state, encoder_outputs)\n                    attn_weights_all.append(attn_weights.cpu().squeeze().numpy())\n\n                    decoder_input = tgt[:, t]\n\n                attn_matrix = np.stack(attn_weights_all, axis=0)\n\n                src_token_ids = src.squeeze().tolist()\n                tgt_token_ids = tgt.squeeze()[1:].tolist()\n\n                src_tokens = [input_vocab.get(tok, \"<unk>\") for tok in src_token_ids]\n                tgt_tokens = [output_vocab.get(tok, \"<unk>\") for tok in tgt_token_ids]\n\n                filtered_src = [(idx, tok) for idx, tok in enumerate(src_tokens) if tok not in special_tokens]\n                filtered_tgt = [(idx, tok) for idx, tok in enumerate(tgt_tokens) if tok not in special_tokens]\n\n                if not filtered_src or not filtered_tgt:\n                    continue\n\n                src_indices, src_labels = zip(*filtered_src)\n                tgt_indices, tgt_labels = zip(*filtered_tgt)\n\n                attn_matrix_filtered = attn_matrix[np.array(tgt_indices)[:, None], np.array(src_indices)]\n\n                ax = axs[count]\n                im = ax.imshow(attn_matrix_filtered, cmap=\"viridis\")\n                ax.set_title(f\"Sample {count+1}\")\n                ax.set_xlabel(\"Input Tokens\")\n                ax.set_ylabel(\"Output Tokens\")\n                ax.set_xticks(np.arange(len(src_labels)))\n                ax.set_yticks(np.arange(len(tgt_labels)))\n                ax.set_xticklabels(src_labels, rotation=90)\n                ax.set_yticklabels(tgt_labels)\n\n                count += 1\n\n            if count >= num_examples:\n                break\n\n    plt.tight_layout()\n\n    # Log the combined image to wandb\n    wandb.log({\"Combined Attention Heatmaps\": wandb.Image(fig)})\n    plt.close(fig)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:28:02.204473Z","iopub.execute_input":"2025-05-20T17:28:02.205144Z","iopub.status.idle":"2025-05-20T17:28:02.217353Z","shell.execute_reply.started":"2025-05-20T17:28:02.205107Z","shell.execute_reply":"2025-05-20T17:28:02.216582Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"wandb.init(project=\"DA6401_A-3_Attention\", name=\"attention-visualization-run\")\nvisualize_attention(model, test_loader, device, latin_idx2token,urdu_idx2token, num_examples=9)\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:30:39.568190Z","iopub.execute_input":"2025-05-20T17:30:39.568887Z","iopub.status.idle":"2025-05-20T17:30:51.880878Z","shell.execute_reply.started":"2025-05-20T17:30:39.568864Z","shell.execute_reply":"2025-05-20T17:30:51.880355Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attention-visualization-run</strong> at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/ux8xczsa' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/ux8xczsa</a><br> View project at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_172930-ux8xczsa/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_173039-xqrfz9ht</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/xqrfz9ht' target=\"_blank\">attention-visualization-run</a></strong> to <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/xqrfz9ht' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/xqrfz9ht</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">attention-visualization-run</strong> at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/xqrfz9ht' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention/runs/xqrfz9ht</a><br> View project at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_A-3_Attention</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_173039-xqrfz9ht/logs</code>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}