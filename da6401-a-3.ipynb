{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11681794,"sourceType":"datasetVersion","datasetId":7331771}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader \n\nfrom tqdm import tqdm\nimport heapq\nimport csv\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\nimport pandas as pd\nimport wandb\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:58:20.236037Z","iopub.execute_input":"2025-05-16T05:58:20.236277Z","iopub.status.idle":"2025-05-16T05:58:27.344028Z","shell.execute_reply.started":"2025-05-16T05:58:20.236258Z","shell.execute_reply":"2025-05-16T05:58:27.343470Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def data_load(path):\n    # input - English\n    # output - Urdu\n    df = pd.read_csv(path,sep='\\t', header=None)\n    input_data = df[1].tolist()\n    output_data = df[0].tolist()\n    return input_data, output_data\n    \ndef create_char_set(train, val):\n    char_set = set()\n    for word in train:\n        for char in str(word):\n            char_set.add(char)\n    for word in val:\n        for char in str(word):\n            char_set.add(char)\n    return char_set\n\ndef check_for_floats(data_list):\n    float_values = []\n    for item in data_list:\n        if isinstance(item, float):\n            float_values.append(item)\n    return float_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:58:56.696827Z","iopub.execute_input":"2025-05-16T05:58:56.697108Z","iopub.status.idle":"2025-05-16T05:58:56.702703Z","shell.execute_reply.started":"2025-05-16T05:58:56.697089Z","shell.execute_reply":"2025-05-16T05:58:56.701958Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_input1, train_output = data_load(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.train.tsv\")\nval_input, val_output = data_load(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.dev.tsv\")\ntest_input, test_output = data_load(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/ur/lexicons/ur.translit.sampled.test.tsv\") \n\nprint(\"Number of training samples: \", len(train_input1))\nprint(\"Number of validation samples: \", len(val_input))\nprint(\"Number of test samples: \", len(test_input))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:00.714042Z","iopub.execute_input":"2025-05-16T05:59:00.714720Z","iopub.status.idle":"2025-05-16T05:59:00.949897Z","shell.execute_reply.started":"2025-05-16T05:59:00.714690Z","shell.execute_reply":"2025-05-16T05:59:00.949071Z"}},"outputs":[{"name":"stdout","text":"Number of training samples:  106260\nNumber of validation samples:  10424\nNumber of test samples:  10517\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"eng_chars = create_char_set(train_input1, val_input)\nprint(\"Total English characters: \",len(eng_chars))\nprint(sorted(eng_chars))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:03.188069Z","iopub.execute_input":"2025-05-16T05:59:03.188834Z","iopub.status.idle":"2025-05-16T05:59:03.220328Z","shell.execute_reply.started":"2025-05-16T05:59:03.188813Z","shell.execute_reply":"2025-05-16T05:59:03.219544Z"}},"outputs":[{"name":"stdout","text":"Total English characters:  26\n['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"ur_chars = create_char_set(train_output, val_output)\nprint(\"Total Urdu characters: \",len(ur_chars))\nprint(sorted(ur_chars))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:04.082716Z","iopub.execute_input":"2025-05-16T05:59:04.083275Z","iopub.status.idle":"2025-05-16T05:59:04.137358Z","shell.execute_reply.started":"2025-05-16T05:59:04.083254Z","shell.execute_reply":"2025-05-16T05:59:04.136804Z"}},"outputs":[{"name":"stdout","text":"Total Urdu characters:  54\n['ء', 'آ', 'ؤ', 'ئ', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ً', 'َ', 'ُ', 'ِ', 'ّ', 'ٗ', 'ٰ', 'ٹ', 'پ', 'چ', 'ڈ', 'ڑ', 'ژ', 'ک', 'گ', 'ں', 'ھ', 'ہ', 'ۃ', 'ی', 'ے', 'ۓ']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"float_values=check_for_floats(train_input1)\nprint(float_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:06.472996Z","iopub.execute_input":"2025-05-16T05:59:06.473583Z","iopub.status.idle":"2025-05-16T05:59:06.482137Z","shell.execute_reply.started":"2025-05-16T05:59:06.473557Z","shell.execute_reply":"2025-05-16T05:59:06.481610Z"}},"outputs":[{"name":"stdout","text":"[nan, nan]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_input = [x for x in train_input1 if not isinstance(x,float)]\nprint(\"Number of training samples: \", len(train_input))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:07.423062Z","iopub.execute_input":"2025-05-16T05:59:07.423748Z","iopub.status.idle":"2025-05-16T05:59:07.433796Z","shell.execute_reply.started":"2025-05-16T05:59:07.423725Z","shell.execute_reply":"2025-05-16T05:59:07.433215Z"}},"outputs":[{"name":"stdout","text":"Number of training samples:  106258\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"max_seq_eng = len(max(train_input+val_input+test_input, key=len))\nmax_seq_ur = len(max(train_output+val_output+test_output, key=len))\nprint(\"Length of the longest English word in corpus:\",max_seq_eng)\nprint(\"Length of the longest Urdu word in corpus::\",max_seq_ur)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:08.137269Z","iopub.execute_input":"2025-05-16T05:59:08.137953Z","iopub.status.idle":"2025-05-16T05:59:08.152711Z","shell.execute_reply.started":"2025-05-16T05:59:08.137929Z","shell.execute_reply":"2025-05-16T05:59:08.151919Z"}},"outputs":[{"name":"stdout","text":"Length of the longest English word in corpus: 21\nLength of the longest Urdu word in corpus:: 14\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\"\"\"eng_chars_idx = {char: idx + 3 for idx, char in enumerate(sorted(eng_chars))}\neng_chars_idx['0'] = 0 # padding\neng_chars_idx['\\t'] = 1 # <SOW>\neng_chars_idx['\\n'] = 2 # <EOW>\nprint(eng_chars_idx)\nur_chars_idx = {char: idx+3 for idx, char in enumerate(sorted(ur_chars))}\nur_chars_idx['0'] = 0 # padding\nur_chars_idx['\\t'] = 1 # <SOW>\nur_chars_idx['\\n'] = 2 # <EOW>\nprint(ur_chars_idx)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:12.332235Z","iopub.execute_input":"2025-05-16T05:59:12.332546Z","iopub.status.idle":"2025-05-16T05:59:12.337970Z","shell.execute_reply.started":"2025-05-16T05:59:12.332525Z","shell.execute_reply":"2025-05-16T05:59:12.337415Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"eng_chars_idx = {char: idx + 3 for idx, char in enumerate(sorted(eng_chars))}\\neng_chars_idx['0'] = 0 # padding\\neng_chars_idx['\\t'] = 1 # <SOW>\\neng_chars_idx['\\n'] = 2 # <EOW>\\nprint(eng_chars_idx)\\nur_chars_idx = {char: idx+3 for idx, char in enumerate(sorted(ur_chars))}\\nur_chars_idx['0'] = 0 # padding\\nur_chars_idx['\\t'] = 1 # <SOW>\\nur_chars_idx['\\n'] = 2 # <EOW>\\nprint(ur_chars_idx)\\n\""},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\n\ndef load_data(input_data, output_data, batch_size=32):\n    \"\"\"\n    Prepares character-level transliteration data for sequence-to-sequence modeling.\n\n    Args:\n        input_data (list or Series): Romanized source words (e.g., Urdu in Latin script).\n        output_data (list or Series): Target words in Devanagari/Urdu script.\n        batch_size (int): Batch size for the DataLoader.\n\n    Returns:\n        dataset (Dataset): Custom PyTorch dataset for transliteration.\n        dataloader (DataLoader): DataLoader for iterating over the dataset.\n        input_vocab (dict): Character-to-index mapping for input characters.\n        target_vocab (dict): Character-to-index mapping for target characters.\n        max_input_len (int): Maximum input sequence length.\n        max_target_len (int): Maximum target sequence length.\n    \"\"\"\n\n    # Determine the maximum lengths for padding\n    max_input_len = len(max(input_data, key=len))\n    max_target_len = len(max(output_data, key=len))\n\n    # Initialize character vocabularies with special tokens\n    input_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n    target_vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}\n    next_index = 3\n\n    # Extract all unique characters from the dataset\n    all_input_chars = ''.join(input_data)\n    all_target_chars = ''.join(output_data)\n\n    # Create input vocabulary\n    for char in sorted(set(all_input_chars)):\n        input_vocab[char] = next_index\n        next_index += 1\n\n    # Reset index for building target vocabulary\n    next_index = 3\n    for char in sorted(set(all_target_chars)):\n        if char not in target_vocab:\n            target_vocab[char] = next_index\n            next_index += 1\n\n    # Tokenize input characters and pad to max_input_len\n    def tokenize_input(word, vocab, max_len):\n        token_ids = [vocab[char] for char in word if char in vocab]\n        padded = token_ids[:max_len] + [vocab['<pad>']] * (max_len - len(token_ids))\n        return torch.tensor(padded)\n\n    # Tokenize target characters with <sos> and <eos>, then pad\n    def tokenize_target(word, vocab, max_len):\n        token_ids = [vocab[char] for char in word if char in vocab]\n        padded = [vocab['<sos>']] + token_ids[:max_len] + [vocab['<eos>']]\n        padded += [vocab['<pad>']] * (max_len + 2 - len(padded))  # +2 for <sos> and <eos>\n        return torch.tensor(padded)\n\n    # Define a custom dataset for transliteration pairs\n    class TransliterationDataset(Dataset):\n        def __init__(self, input_words, target_words, input_vocab, target_vocab, max_input_len, max_target_len):\n            self.input_words = input_words\n            self.target_words = target_words\n            self.input_vocab = input_vocab\n            self.target_vocab = target_vocab\n            self.max_input_len = max_input_len\n            self.max_target_len = max_target_len\n\n        def __len__(self):\n            return len(self.input_words)\n\n        def __getitem__(self, idx):\n            input_word = self.input_words[idx]\n            target_word = self.target_words[idx]\n\n            # Convert to tensor of indices\n            input_tensor = tokenize_input(input_word, self.input_vocab, self.max_input_len)\n            target_tensor = tokenize_target(target_word, self.target_vocab, self.max_target_len)\n\n            return input_tensor, target_tensor\n\n    # Create dataset and dataloader\n    dataset = TransliterationDataset(input_data, output_data,\n                                     input_vocab, target_vocab,\n                                     max_input_len, max_target_len)\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    return dataset, dataloader, input_vocab, target_vocab, max_input_len, max_target_len\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:12.883350Z","iopub.execute_input":"2025-05-16T05:59:12.883641Z","iopub.status.idle":"2025-05-16T05:59:12.897475Z","shell.execute_reply.started":"2025-05-16T05:59:12.883622Z","shell.execute_reply":"2025-05-16T05:59:12.896893Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset, dataloader, input_vocab, target_vocab, max_input_len, max_target_len = load_data(train_input+val_input+test_input, train_output+val_output+test_output,batch_size = 64)\nprint('All English Characters:\\n',input_vocab,'\\n All Urdu Characters:\\n', target_vocab,'\\n Length of  longest English word:', max_input_len,'\\n Length of  longest Urdu word:', max_target_len) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:15.872122Z","iopub.execute_input":"2025-05-16T05:59:15.872748Z","iopub.status.idle":"2025-05-16T05:59:15.928228Z","shell.execute_reply.started":"2025-05-16T05:59:15.872728Z","shell.execute_reply":"2025-05-16T05:59:15.927527Z"}},"outputs":[{"name":"stdout","text":"All English Characters:\n {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28} \n All Urdu Characters:\n {'<pad>': 0, '<sos>': 1, '<eos>': 2, 'ء': 3, 'آ': 4, 'ؤ': 5, 'ئ': 6, 'ا': 7, 'ب': 8, 'ت': 9, 'ث': 10, 'ج': 11, 'ح': 12, 'خ': 13, 'د': 14, 'ذ': 15, 'ر': 16, 'ز': 17, 'س': 18, 'ش': 19, 'ص': 20, 'ض': 21, 'ط': 22, 'ظ': 23, 'ع': 24, 'غ': 25, 'ف': 26, 'ق': 27, 'ك': 28, 'ل': 29, 'م': 30, 'ن': 31, 'ه': 32, 'و': 33, 'ي': 34, 'ً': 35, 'َ': 36, 'ُ': 37, 'ِ': 38, 'ّ': 39, 'ٗ': 40, 'ٰ': 41, 'ٹ': 42, 'پ': 43, 'چ': 44, 'ڈ': 45, 'ڑ': 46, 'ژ': 47, 'ک': 48, 'گ': 49, 'ں': 50, 'ھ': 51, 'ہ': 52, 'ۃ': 53, 'ی': 54, 'ے': 55, 'ۓ': 56} \n Length of  longest English word: 21 \n Length of  longest Urdu word: 14\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Seq2Seq Model","metadata":{}},{"cell_type":"code","source":"# Encoder: Encodes input sequence into context vector\nclass Encoder(nn.Module):\n    def __init__(self, input_vocab_size, hidden_size, embedding_dim,\n                 num_layers=1, dropout=0.5, cell_type='gru', bidirectional=False):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n\n        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n        self.dropout = nn.Dropout(dropout)\n\n        rnn_cls = {'lstm': nn.LSTM, 'gru': nn.GRU, 'rnn': nn.RNN}[cell_type]\n        self.rnn = rnn_cls(\n            embedding_dim, hidden_size, num_layers,\n            dropout=dropout, bidirectional=bidirectional, batch_first=True\n        )\n\n    def forward(self, input_seq):\n        # Embed and apply dropout\n        embedded = self.dropout(self.embedding(input_seq))\n\n        # Pass through RNN\n        rnn_output, hidden_state = self.rnn(embedded)\n\n        # Handle LSTM: return both hidden and cell state\n        if self.cell_type == 'lstm':\n            hidden, cell = hidden_state\n            if self.bidirectional:\n                # Combine forward and backward hidden states by summing\n                return torch.sum(hidden[-2:], dim=0, keepdim=True), torch.sum(cell[-2:], dim=0, keepdim=True)\n            else:\n                return hidden[-1].unsqueeze(0), cell[-1].unsqueeze(0)\n        else:\n            # GRU or RNN\n            if self.bidirectional:\n                return torch.sum(hidden_state[-2:], dim=0, keepdim=True)\n            else:\n                return hidden_state[-1].unsqueeze(0)\n\n\n# Decoder: Generates output sequence using encoder context\nclass Decoder(nn.Module):\n    def __init__(self, hidden_size, embedding_dim, output_vocab_size,\n                 num_layers=1, dropout=0.5, cell_type='gru'):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cell_type = cell_type\n\n        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)\n        self.dropout = nn.Dropout(dropout)\n\n        rnn_cls = {'lstm': nn.LSTM, 'gru': nn.GRU, 'rnn': nn.RNN}[cell_type]\n        self.rnn = rnn_cls(\n            embedding_dim, hidden_size, num_layers,\n            dropout=dropout, batch_first=True\n        )\n\n        self.fc = nn.Linear(hidden_size, output_vocab_size)\n\n    def forward(self, input_token, hidden_state):\n        # Add time dimension for RNN\n        input_token = input_token.unsqueeze(1)\n\n        # Embed input token and apply dropout\n        embedded = self.dropout(self.embedding(input_token))\n\n        # Pass through decoder RNN\n        rnn_output, new_hidden_state = self.rnn(embedded, hidden_state)\n\n        # Predict next token from output\n        logits = self.fc(rnn_output)\n\n        return logits.squeeze(1), new_hidden_state\n\n\n# Sequence-to-Sequence: Combines encoder and decoder for full translation\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_vocab_size, output_vocab_size, hidden_size, embedding_dim,\n                 encoder_layers=1, decoder_layers=1, dropout=0.3, cell_type='gru', bidirectional=True):\n        super(Seq2Seq, self).__init__()\n        self.encoder = Encoder(input_vocab_size, hidden_size, embedding_dim,\n                               encoder_layers, dropout, cell_type, bidirectional)\n        self.decoder = Decoder(hidden_size, embedding_dim, output_vocab_size,\n                               decoder_layers, dropout, cell_type)\n        self.output_vocab_size = output_vocab_size\n\n    def forward(self, input_seq, target_seq, teacher_forcing_ratio=0.5):\n        batch_size = input_seq.size(0)\n        target_seq_len = target_seq.size(1)\n\n        outputs = torch.zeros(batch_size, target_seq_len, self.output_vocab_size).to(input_seq.device)\n\n        # Encode the input sequence\n        encoder_hidden = self.encoder(input_seq)\n\n        # Initialize decoder hidden state from encoder\n        decoder_hidden = self._init_decoder_hidden(encoder_hidden)\n\n        # First decoder input is <sos>\n        decoder_input = target_seq[:, 0]\n\n        for t in range(1, target_seq_len):\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n            outputs[:, t] = decoder_output\n\n            # Teacher forcing: use actual target or model prediction\n            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n            next_input = target_seq[:, t] if use_teacher_forcing else decoder_output.argmax(1)\n            decoder_input = next_input\n\n        return outputs\n\n    def _init_decoder_hidden(self, encoder_hidden):\n        \"\"\"Adjust encoder output to match decoder initial state shape.\"\"\"\n        decoder_layers = self.decoder.num_layers\n\n        if self.decoder.cell_type == 'lstm':\n            hidden, cell = encoder_hidden\n            hidden_layers = hidden.shape[0]\n\n            # Pad or trim hidden/cell states to match decoder's expected layers\n            if hidden_layers < decoder_layers:\n                pad_size = decoder_layers - hidden_layers\n                hidden = torch.cat([hidden, torch.zeros(pad_size, *hidden.shape[1:], device=hidden.device)], dim=0)\n                cell = torch.cat([cell, torch.zeros(pad_size, *cell.shape[1:], device=cell.device)], dim=0)\n            else:\n                hidden = hidden[:decoder_layers]\n                cell = cell[:decoder_layers]\n            return (hidden, cell)\n\n        else:\n            # GRU or RNN\n            hidden_layers = encoder_hidden.shape[0]\n            hidden = encoder_hidden\n            if hidden_layers < decoder_layers:\n                pad_size = decoder_layers - hidden_layers\n                hidden = torch.cat([hidden, torch.zeros(pad_size, *hidden.shape[1:], device=hidden.device)], dim=0)\n            else:\n                hidden = hidden[:decoder_layers]\n            return hidden\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:19.084082Z","iopub.execute_input":"2025-05-16T05:59:19.084785Z","iopub.status.idle":"2025-05-16T05:59:19.107208Z","shell.execute_reply.started":"2025-05-16T05:59:19.084765Z","shell.execute_reply":"2025-05-16T05:59:19.106139Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Train and Evaluate","metadata":{}},{"cell_type":"code","source":"# Training function: Trains model for one epoch\ndef train(model, dataloader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    for latin_inputs, urdu_targets in dataloader:\n        latin_inputs = latin_inputs.to(device)\n        urdu_targets = urdu_targets.to(device)\n\n        optimizer.zero_grad()\n        output = model(latin_inputs, urdu_targets)\n        output_dim = output.shape[-1]\n\n        output_flat = output.view(-1, output_dim)\n        targets_flat = urdu_targets.view(-1)\n\n        loss = criterion(output_flat, targets_flat)\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n\n        # Compute accuracy (character-level)\n        _, predictions = torch.max(output, dim=2)\n        correct = (predictions == urdu_targets).sum().item()\n        total_correct += correct\n        total_samples += urdu_targets.size(0)\n\n    accuracy = total_correct / total_samples\n    return model, total_loss / len(dataloader), accuracy\n\n\n\n# Evaluation function: Evaluates model performance on validation/test data\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch_latin, batch_urdu in dataloader:\n            batch_latin = batch_latin.to(device)\n            batch_urdu = batch_urdu.to(device)\n\n            # Forward pass with no teacher forcing during evaluation\n            predictions = model(batch_latin, batch_urdu, teacher_forcing_ratio=0.0)\n            vocab_size = predictions.shape[-1]\n\n            # Compute loss\n            loss = criterion(predictions.view(-1, vocab_size), batch_urdu.view(-1))\n            total_loss += loss.item()\n\n            # Get predicted token indices\n            _, predicted_indices = torch.max(predictions, dim=2)\n\n            # Optional: Adjust predictions if vocab has special token offsets\n            invalid_token_mask = predicted_indices > 9\n            predicted_indices[invalid_token_mask] -= 2\n\n            # Compare full sequences for exact word match\n            correct_predictions = (predicted_indices == batch_urdu).sum().item()\n\n            total_correct += correct_predictions\n            total_samples += batch_urdu.size(0)\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = (total_correct / total_samples)\n\n    return avg_loss, accuracy\n\n\n# Model configuration\ninput_vocab_size = 26         # Number of Latin script characters\noutput_vocab_size = 54        # Number of Urdu script characters\nembedding_dim = 128           # Embedding dimension for both encoder and decoder\nhidden_size = 128             # Hidden state size for RNN cells\nencoder_layers = 3            # Number of layers in encoder RNN\ndecoder_layers = 2            # Number of layers in decoder RNN\ncell_type = 'lstm'            # RNN cell type: 'rnn', 'gru', or 'lstm'\nbatch_size = 64               # Batch size during training\nnum_epochs = 20               # Total number of training epochs\ndropout = 0.2                 # Dropout probability\nlearning_rate = 0.001         # Learning rate for optimizer\nbidirectional = True          # Whether the encoder is bidirectional\n\n# Initialize model, loss function, and optimizer\nmodel = Seq2Seq(input_vocab_size, output_vocab_size, hidden_size, embedding_dim,\n                encoder_layers, decoder_layers, dropout, cell_type, bidirectional)\n\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:41.094518Z","iopub.execute_input":"2025-05-16T05:59:41.094799Z","iopub.status.idle":"2025-05-16T05:59:41.140956Z","shell.execute_reply.started":"2025-05-16T05:59:41.094778Z","shell.execute_reply":"2025-05-16T05:59:41.140375Z"}},"outputs":[{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(26, 128)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (rnn): LSTM(128, 128, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(54, 128)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (rnn): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.2)\n    (fc): Linear(in_features=128, out_features=54, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"wandb.login(key='53b259076c07d0811d73bf26bfef7437e04dbf66')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:43.344937Z","iopub.execute_input":"2025-05-16T05:59:43.345215Z","iopub.status.idle":"2025-05-16T05:59:49.378413Z","shell.execute_reply.started":"2025-05-16T05:59:43.345195Z","shell.execute_reply":"2025-05-16T05:59:49.377862Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma23m013\u001b[0m (\u001b[33mma23m013-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Sweep configuration for hyperparameter optimization using Weights & Biases\nsweep_config = {\n    'method': 'bayes',  # Bayesian optimization for more efficient search\n    'metric': {\n        'name': 'val_accuracy',  # Metric to optimize\n        'goal': 'maximize'       # Maximize validation accuracy\n    },\n    'parameters': {\n        'embedding_size': {\n            'values': [16, 32, 64, 128, 256]  # Size of embedding vectors\n        },\n        'dropout': {\n            'values': [0.2, 0.3, 0.5]  # Dropout probability to reduce overfitting\n        },\n        'encoder_layers': {\n            'values': [1, 2, 3]  # Number of RNN layers in the encoder\n        },\n        'decoder_layers': {\n            'values': [1, 2, 3]  # Number of RNN layers in the decoder\n        },\n        'hidden_size': {\n            'values': [16, 32, 64, 128, 256]  # Hidden state dimension in RNN\n        },\n        'rnn_cell_type': {\n            'values': ['lstm', 'gru', 'rnn']  # RNN cell type to use\n        },\n        'use_bidirectional_encoder': {\n            'values': [True, False]  # Whether encoder is bidirectional\n        },\n        'batch_size': {\n            'values': [32, 64]  # Batch size during training\n        },\n        'num_epochs': {\n            'values': [10, 12]  # Number of training epochs\n        },\n        'learning_rate': {\n            'values': [0.01, 0.001]  # Learning rate for optimizer\n        }\n    }\n}\n\n# Launch sweep on Weights & Biases\nsweep_id = wandb.sweep(sweep=sweep_config, project='DA6401_Assignment-3')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T05:59:57.175969Z","iopub.execute_input":"2025-05-16T05:59:57.176806Z","iopub.status.idle":"2025-05-16T05:59:57.503086Z","shell.execute_reply.started":"2025-05-16T05:59:57.176784Z","shell.execute_reply":"2025-05-16T05:59:57.502296Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: rqyz759c\nSweep URL: https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/sweeps/rqyz759c\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def run_training():\n    '''\n    This function is executed by WandB for each set of hyperparameters during the sweep.\n    It initializes the model using the current configuration, trains and evaluates it,\n    and logs the metrics to Weights & Biases.\n    '''\n    with wandb.init() as run:\n        # Create a descriptive run name using the current hyperparameters\n        run_name = (\n            f\"cell-{wandb.config.rnn_cell_type}\"\n            f\"_encLayers-{wandb.config.encoder_layers}\"\n            f\"_decLayers-{wandb.config.decoder_layers}\"\n            f\"_dropout-{wandb.config.dropout}\"\n            f\"_embedSize-{wandb.config.embedding_size}\"\n            f\"_hiddenSize-{wandb.config.hidden_size}\"\n            f\"_batchSize-{wandb.config.batch_size}\"\n            f\"_epochs-{wandb.config.num_epochs}\"\n            f\"_lr-{wandb.config.learning_rate}\"\n        )\n        wandb.run.name = run_name\n\n        # Instantiate model\n        model = Seq2Seq(\n            input_vocab_size=30,\n            output_vocab_size=60,\n            hidden_size=wandb.config.hidden_size,\n            embedding_dim=wandb.config.embedding_size,\n            encoder_layers=wandb.config.encoder_layers,\n            decoder_layers=wandb.config.decoder_layers,\n            dropout=wandb.config.dropout,\n            cell_type=wandb.config.rnn_cell_type,\n            bidirectional=wandb.config.use_bidirectional_encoder\n        )\n        print(model)\n\n        # Setup\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model.to(device)\n\n        # Load data\n        _, train_loader, _, _, _, _ = load_data(train_input,train_output, batch_size=wandb.config.batch_size)\n\n        _, val_loader, _, _, _, _ = load_data(val_input,val_output, batch_size=wandb.config.batch_size)\n\n        # Train & evaluate\n        for epoch in range(wandb.config.num_epochs):\n            model, train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n            val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n\n            # Log metrics\n            wandb.log({\n                'Epoch': epoch,\n                'Train Loss': train_loss,\n                'Train Accuracy (%)': train_accuracy,\n                'Validation Loss': val_loss,\n                'Validation Accuracy (%)': val_accuracy\n            })\n\n            print(f\"Epoch {epoch + 1}/{wandb.config.num_epochs} | \"\n                  f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}% | \"\n                  f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n\n# Start the sweep\nwandb.agent(sweep_id, function=run_training, count=1)\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T06:00:07.012923Z","iopub.execute_input":"2025-05-16T06:00:07.013629Z","iopub.status.idle":"2025-05-16T06:10:19.176773Z","shell.execute_reply.started":"2025-05-16T06:00:07.013608Z","shell.execute_reply":"2025-05-16T06:10:19.176245Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cn31sxzl with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 12\n\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_cell_type: gru\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidirectional_encoder: True\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250516_060013-cn31sxzl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/runs/cn31sxzl' target=\"_blank\">mild-sweep-1</a></strong> to <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/sweeps/rqyz759c' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/sweeps/rqyz759c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/sweeps/rqyz759c' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/sweeps/rqyz759c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/runs/cn31sxzl' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/runs/cn31sxzl</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(30, 128)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (rnn): GRU(128, 32, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(60, 128)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (rnn): GRU(128, 32, num_layers=3, batch_first=True, dropout=0.2)\n    (fc): Linear(in_features=32, out_features=60, bias=True)\n  )\n)\nEpoch 1/12 | Train Loss: 1.4040 | Train Accuracy: 10.46% | Val Loss: 2.1845 | Val Accuracy: 7.76%\nEpoch 2/12 | Train Loss: 1.3741 | Train Accuracy: 10.57% | Val Loss: 3.4213 | Val Accuracy: 0.01%\nEpoch 3/12 | Train Loss: 1.3403 | Train Accuracy: 10.64% | Val Loss: 2.2166 | Val Accuracy: 7.60%\nEpoch 4/12 | Train Loss: 1.3445 | Train Accuracy: 10.60% | Val Loss: 2.1453 | Val Accuracy: 7.72%\nEpoch 5/12 | Train Loss: 1.3271 | Train Accuracy: 10.64% | Val Loss: 3.3456 | Val Accuracy: 0.01%\nEpoch 6/12 | Train Loss: 1.3305 | Train Accuracy: 10.62% | Val Loss: 3.4136 | Val Accuracy: 0.01%\nEpoch 7/12 | Train Loss: 1.3478 | Train Accuracy: 10.62% | Val Loss: 2.3292 | Val Accuracy: 7.24%\nEpoch 8/12 | Train Loss: 1.3373 | Train Accuracy: 10.62% | Val Loss: 2.2491 | Val Accuracy: 7.42%\nEpoch 9/12 | Train Loss: 1.3421 | Train Accuracy: 10.60% | Val Loss: 2.1968 | Val Accuracy: 7.95%\nEpoch 10/12 | Train Loss: 1.3640 | Train Accuracy: 10.55% | Val Loss: 3.3293 | Val Accuracy: 0.02%\nEpoch 11/12 | Train Loss: 1.3552 | Train Accuracy: 10.57% | Val Loss: 3.5122 | Val Accuracy: 0.01%\nEpoch 12/12 | Train Loss: 1.3633 | Train Accuracy: 10.55% | Val Loss: 2.1805 | Val Accuracy: 8.01%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Train Accuracy (%)</td><td>▁▅█▇█▇▇▇▇▅▅▅</td></tr><tr><td>Train Loss</td><td>█▅▂▃▁▁▃▂▂▄▄▄</td></tr><tr><td>Validation Accuracy (%)</td><td>█▁██▁▁▇▇█▁▁█</td></tr><tr><td>Validation Loss</td><td>▁█▁▁▇▇▂▂▁▇█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>11</td></tr><tr><td>Train Accuracy (%)</td><td>10.55283</td></tr><tr><td>Train Loss</td><td>1.36328</td></tr><tr><td>Validation Accuracy (%)</td><td>8.01391</td></tr><tr><td>Validation Loss</td><td>2.18053</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell-gru_encLayers-3_decLayers-3_dropout-0.2_embedSize-128_hiddenSize-32_batchSize-64_epochs-12_lr-0.01</strong> at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/runs/cn31sxzl' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3/runs/cn31sxzl</a><br> View project at: <a href='https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3' target=\"_blank\">https://wandb.ai/ma23m013-iit-madras/DA6401_Assignment-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250516_060013-cn31sxzl/logs</code>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}